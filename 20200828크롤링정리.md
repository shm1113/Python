### :book: PYPI(Python Package Index) : 파이썬 관련 패키지들이 모여있는 저장소

# 크롤링





## 네이버 영화 

```python
from bs4 import BeautifulSoup

import urllib.request
#파이썬 내부에 기본적으로 존재하는 패키지 

resp = urllib.request.urlopen('https://movie.naver.com/movie/running/current.nhn')
#print(resp)
#실행결과 :<http.client.HTTPResponse object at 0x0000029875831FD0> 데이터에 대한 객체를 반환

soup = Beautifulsoup(resp,'html.parser') #가져온 데이터를 Parsing해주는 작업이다 

movies=soup.find_all('dl',class_='lst_dsc')#크롤링할 것들을 가져온다

#제목이랑 별점 출력하기
star = soup.find_all('span',class='num')
title=soup.find_all('p',class_='rank_tx')

#for문을 통해서 하나하나씩 꺼내기 
for movie in movies:
    title = movie.find('a').text #a태그 제일 위애 있는 것 가지고 올 것이다.
    star = movie.find('span', class_='num').text
    print('[제목: {} , 별점 : {}]'.format(title,star))
    

#실행결과
[제목: 테넷 , 별점 : 8.54]
[제목: 다만 악에서 구하소서 , 별점 : 7.79]
[제목: 오케이 마담 , 별점 : 6.60]
[제목: 극장판 짱구는 못말려: 신혼여행 허리케인~ 사라진 아빠! , 별점 : 9.19]
[제목: 캐리비안 해적과 마법 다이아몬드 , 별점 : 5.50]
[제목: 남매의 여름밤 , 별점 : 9.12]
[제목: 강철비2: 정상회담 , 별점 : 5.04]
...
```

#### **`urlopen()`**

-   urlopen함수의 인자는 데이터를 얻고 싶은 웹페이지의 주소를 주면 되

#### `import urllib.request()`

-   python 내부에 기본적으로 가지고 있는 패키지로 원하는 해당 페이지한테 요청(GET || POST)을 보내고 서버로부터 response객체를 응답받는다





## find()와 find_all() 차이

-   ### find_all(): 해당 조건에 맞는 모든 요소를 가져온다

-   #### find() : 해당 조건에 맞는 하나의 요소만 가져온다

## select('선택자')

>   CSS선택자를 사용해서 원하는 요소값을 가져 올 수 있다
>
>   -   #### `find('태그명', class_='클래스명')`
>
>   -   #### `select('태그명.클래스명')`
>
>   -   #### `select('태그명')`
>
>   -   #### `elect('.클래스명')`
>
>   -   #### `slect('상위태그명 > 하위태그명 > 하위태그명')`
>
>   -   #### `select('상위태그명.클래스명 > 하위태그명.클래스명')` :바로 아래의(자식)태그를 선택시에는 > 기호를 사용한다.
>
>   -   #### `select('상위태그명.클래스명 하~~위태그명')`: 아래의(자손)태그를 선택시에는 띄어쓰기 사용한다.
>
>   -   #### `select('상위태그명 > 바로아래태그명 하~~위태그명')`
>
>       #### `select('.클래스명')`
>
>   -   #### `select('#아이디명')`
>
>   -   #### `select('#아이디명 > 태그명.클래스명')`
>
>   -   #### `select('태그명[속성1=값1]')`



## 네이버 웹툰

```python
from bs4 import BeautifulSoup
import requests
import json
from ctypes.test.test_as_parameter import dll

#크롤링 할 페이지 requests로 가져온다. (네이버 월요웹툰)
resp = requests.get('https://comic.naver.com/webtoon/weekdayList.nhn?week=mon')
#print(resp)
#실행결과
#<Response [200]> get은 단순히 응답코드만 가지고온다.

#html내용을 가지고 오기위해 text를 가지고와서 / parsing해준다.
#html = resp.text
#soup=BeautifulSoup(html,'html.parser')
#밑에 한 줄 코드와 같은 뜻 
soup = BeautifulSoup(resp.text,'html.parser')

#월요웹툰 제목 , 별점출력하기
webtoons= soup.find('ul', {'class': 'img_list'}) #속성으로 가지고옴
#webtoons2=soup.find_all('ul',class_='img_list') #[여기 안에 나옴]
dl = webtoons.select('dl') #select태그 : 속성으로 가지고 오지 않은 것은 안된다

lst = list() #배열 생성자
for chd in dl:
    title = chd.find('a')['title']
    star = chd.find('strong').text
    #print('{}[{}]'.format(title, star)) 
    
    tmp={} #딕셔너리 생성자
    tmp['title'] =title
    tmp['star']=star
    #{'title' : '제목' ,'star' : 0.00}
    lst.append(tmp)
#print(lst) 리스트에 딕셔너리가 들어가있다. 딕셔너리를 리스트로 관리한다는 말!
#실행결과
#[{'title': '인생존망', 'star': '9.82'}, {'title': '뷰티풀 군바리', 'star': '9.82'}, ...]

res = {} #딕셔너리 생성자
res['webtoons'] = lst
#실행결과 : json형태의 딕셔너리
#{'webtoons': [{'title': '인생존망', 'star': '9.82'}, {'title': '뷰티풀 군바리', 'star': '9.82'},...]
print(res)

#제이슨 객체로 저장하기
res_json = json.dumps(res,ensure_ascii=False) #dumps : 바꾸는 것
print(res_json)
#실행결과
#{'webtoons': [{'title': '인생존망', 'star': '9.82'}, {'title': '뷰티풀 군바리', 'star': '9.82'}, ...]

#패키지 새로고침하면 json파일이 생성되어있다.
with open('webtoons2.json', 'w', encoding='utf-8') as f:
    f.write(res_json)
```





## 스타벅스 매장검색 크롤링

```python
# -*- coding:utf-8 -*-

import requests
import json

#https://www.starbucks.co.kr/store/store_map.do -> STORE -> 매장찾기 -> 지역검색

def getSido():
    url='https://www.starbucks.co.kr/store/getSidoList.do'
    resp = requests.post(url)
    #print(resp.json())(['list'])
    sido_json = resp.json()['list']
    
    #sido_json에 있는 값들을 x에 넣어서 
    #그 x에서sido_cd에 대한 value를 가지고와 
    #그걸 리스트로바꾸자
    sido_code=list(map(lambda x: x['sido_cd'], sido_json))
    #print(sido_code) 
    #실행결과
    #['01', '08', '02', '03', '04', '05', '06', '07', '09', '10', '11', '12', '13', '14', '15', '16', '17']
    
    sido_name=list(map(lambda x:x['sido_nm'], sido_json))
    #print(sido_name)
    #실행결과
    #['서울', '경기', '광주', '대구', '대전', '부산', '울산', '인천', '강원', '경남', '경북', '전남', '전북', '충남', '충북', '제주', '세종']
    
    #zip함수! 확실히 알아둘 것
    sido_dict = dict(zip(sido_code,sido_name))
    #print(sido_dict)
    #실행결과
    #{'01': '서울', '08': '경기', '02': '광주', '03': '대구', '04': '대전', '05': '부산', '06': '울산', '07': '인천', '09': '강원', '10': '경남', '11': '경북', '12': '전남', '13': '전북', '14': '충남', '15': '충북', '16': '제주', '17': '세종'}
    
    return sido_dict
def getGuGun(sido_code):
    url='https://www.starbucks.co.kr/store/getGugunList.do'
    resp = requests.post(url,data={'sido_cd' : sido_code})
    #print(resp.json)
    #실행결과
    #<bound method Response.json of <Response [200]>>
    
    gugun_json= resp.json()['list']
    #print(gugun_json)
    #실행결과
    #[{'seq': 0, 'sido_cd': None, 'sido_nm': None, 'gugun_cd': '0101', 'gugun_nm': '강남구', ...]
    
    #gugun_json에 있는 값들 x에 넣고 그 x에 대한 gugun_cd를 가지고온다 ->리스트로 바꾼다 
    #gugun_json에 있는 값들 x에 넣고 그 x에 대한 gugun_nm를 가지고와서 리스트로 바꿔 
    #이 두개를 합쳐
    gugun_dict=dict(zip(list(map(lambda x:x['gugun_cd'], gugun_json)),list(map(lambda x:x['gugun_nm'], gugun_json))))
    print(gugun_dict)
    
def getStore(sido_code='', gugun_code=''):
    url='https://www.starbucks.co.kr/store/getStore.do'
    resp = requests.post(url, data={
                            'ins_lat' : '37.56682',
                            'ins_lng': '126.97865',
                            'p_sido_cd' : sido_code,
                            'p_gugun_cd' : gugun_code,
                            'in_biz_cd':'',
                            'set_date': ''
        })   
    print(resp.json())
    print(resp.json()['list'])
    
    
#s_name, tel, doro_address, lat, lot -> json으로 만들자
    store_json = resp.json()['list']
    store_list=list()
    count=0
    for store in store_json:
            store_dict=dict()
            store_dict['s_name'] = store['s_name']
            store_dict['tel'] = store['tel']
            store_dict['doro_address'] = store['doro_address']
            store_dict['latitude'] = store['lat']
            store_dict['longtitude'] = store['lot']
            print(store_dict)
            store_list.append(store_dict)
            count += 1
    print(count)
    store_result = dict()
    store_result['store_list'] = store_list
    store_result['count'] = count
    result = json.dumps(store_result, ensure_ascii=False)
    return result
        
        
        
        
        
if __name__=='__main__':
    print(getSido())
    sido=input('도시 코드를 입력하세요 ! : ')
    print(getGuGun(sido))
    gugun = input('구군 코드를 입력하세요 ! : ')
    print(getStore(gugun_code=gugun))
    
```



## 인스타그램 크롤링



>   ```python
>   #-*-coding:utf-8 -*-
>   
>   
>   from selenium import webdriver
>   from bs4 import BeautifulSoup
>   
>   url= 'https://www.instagram.com/explore/tags/'+input('search keyword:')
>   
>   driver= webdriver.Chrome('C:/test/chromedriver.exe')
>   driver.implicitly_wait(3) #3초기다리겠다
>   driver.get(url)
>   
>   soup=BeautifulSoup(driver.page_source,'html.parser')
>   img_list=soup.find_all('div',{'class':'KL4Bh'})
>   #print(img_list) #div(KL4Bh)값들
>   for img in img_list:
>       print(img.find('img')["src"])
>   
>   ```

인스타는 ajax를 사용했기 때문에 브라우저에서 화면이 전부 랜더링 된 후에 크롤링을 실행해야하는데 이것을 도와주는 것이 selenium이다

## selenium

>   인스타는 ajax를 사용했기 때문에 브라우저에서 화면이 전부 랜더링 된 후에 크롤링을 실행해야하는데 이것을 도와주는 것이 selenium이다
>
>   -   ajax가 동작한 후에 크롤링해올수 있도록 해준다
>
>   -   java,python,c#,ruby,javascript,kotlin 다양한 언어에서 사용가능
>
>
>   #### 설치
>
>   -   cmd - pip install selenium
>
>   ####  WebDriver
>
>   -   Selenium은 webdriver라는 것을 통해 디바이스에 설치된 브라우저들을 제어할 수 있다.
>
>   #### Chrome WebDriver
>
>   -   크롬을 제어하기위해 설치해주는 WevDriver 



>   